{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LOADING AND PREPARING THE DATA ##","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:25.407341Z","iopub.execute_input":"2023-04-30T17:52:25.407846Z","iopub.status.idle":"2023-04-30T17:52:28.328870Z","shell.execute_reply.started":"2023-04-30T17:52:25.407799Z","shell.execute_reply":"2023-04-30T17:52:28.325785Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#open the training file and assign to variable text\nwith open(\"/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv\", \"r\", encoding=\"utf-8\") as f:\n    train_text = f.read()\n    \nwith open(\"/kaggle/input/the-bards-best-a-character-modeling-dataset/validation.csv\", \"r\", encoding=\"utf-8\") as t:\n    val_text = t.read()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-30T17:52:28.332383Z","iopub.execute_input":"2023-04-30T17:52:28.335749Z","iopub.status.idle":"2023-04-30T17:52:28.367987Z","shell.execute_reply.started":"2023-04-30T17:52:28.335645Z","shell.execute_reply":"2023-04-30T17:52:28.366963Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"The length of the file is \", len(train_text))","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.369645Z","iopub.execute_input":"2023-04-30T17:52:28.370411Z","iopub.status.idle":"2023-04-30T17:52:28.376232Z","shell.execute_reply.started":"2023-04-30T17:52:28.370373Z","shell.execute_reply":"2023-04-30T17:52:28.375171Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The length of the file is  1003862\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_text[:500])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.378932Z","iopub.execute_input":"2023-04-30T17:52:28.379632Z","iopub.status.idle":"2023-04-30T17:52:28.387835Z","shell.execute_reply.started":"2023-04-30T17:52:28.379595Z","shell.execute_reply":"2023-04-30T17:52:28.386459Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"text\n\"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounte\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## TOKENIZATION AND ENCODING/DECODING ##","metadata":{}},{"cell_type":"markdown","source":"#### Tokenizing the characters ####","metadata":{}},{"cell_type":"code","source":"chars = sorted(list(set(train_text)))#assorted list of characters\nvocab_size = len(chars)\nprint(\"\".join(chars))#print characters\nprint(vocab_size)#number of characters","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.389410Z","iopub.execute_input":"2023-04-30T17:52:28.389782Z","iopub.status.idle":"2023-04-30T17:52:28.413045Z","shell.execute_reply.started":"2023-04-30T17:52:28.389746Z","shell.execute_reply":"2023-04-30T17:52:28.411982Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n !\"$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n66\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Decoding and encoding functions ####","metadata":{}},{"cell_type":"code","source":"\nstoi = { ch:i for i,ch in enumerate(chars)}#dictionary for string to integer\nitos = { i:ch for i,ch in enumerate(chars)}#dictionary for integer to string\nencode = lambda s: [stoi[c] for c in s]#function for string -> int\ndecode = lambda l: \"\".join([itos[i] for i in l])#function for int -> string\n\n#EXAMPLE\nprint(encode(\"transformer\"))\nprint(decode(encode(\"transformer\")))","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.414636Z","iopub.execute_input":"2023-04-30T17:52:28.415367Z","iopub.status.idle":"2023-04-30T17:52:28.426730Z","shell.execute_reply.started":"2023-04-30T17:52:28.415326Z","shell.execute_reply":"2023-04-30T17:52:28.425585Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[59, 57, 40, 53, 58, 45, 54, 57, 52, 44, 57]\ntransformer\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Creating tensors out of text sequences ####","metadata":{}},{"cell_type":"code","source":"train_data = torch.tensor(encode(train_text), dtype= torch.long)#tensor of all encoded training data \nval_data = torch.tensor(encode(val_text), dtype= torch.long)#same for validation data\n\n#VISUALIZING AN EXAMPLE\nprint(train_data.shape, train_data.dtype)\nprint(train_data[:100])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.428226Z","iopub.execute_input":"2023-04-30T17:52:28.429265Z","iopub.status.idle":"2023-04-30T17:52:28.745737Z","shell.execute_reply.started":"2023-04-30T17:52:28.429219Z","shell.execute_reply":"2023-04-30T17:52:28.744126Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([1003862]) torch.int64\ntensor([59, 44, 63, 59,  0,  3, 19, 48, 57, 58, 59,  1, 16, 48, 59, 48, 65, 44,\n        53, 11,  0, 15, 44, 45, 54, 57, 44,  1, 62, 44,  1, 55, 57, 54, 42, 44,\n        44, 43,  1, 40, 53, 64,  1, 45, 60, 57, 59, 47, 44, 57,  7,  1, 47, 44,\n        40, 57,  1, 52, 44,  1, 58, 55, 44, 40, 50,  9,  0,  0, 14, 51, 51, 11,\n         0, 32, 55, 44, 40, 50,  7,  1, 58, 55, 44, 40, 50,  9,  0,  0, 19, 48,\n        57, 58, 59,  1, 16, 48, 59, 48, 65, 44])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# THE MODEL #","metadata":{}},{"cell_type":"markdown","source":"#### Defining some Variables ####","metadata":{}},{"cell_type":"code","source":"\nblock_size = 8 #the size of the \"chunks\" passed through the transformer\nbatch_size = 4 #how many blocks will be processed at a time\nn_embd = 32 #size of vector embeddings for each token\neval_iters=500\neval_interval=100\nhead_size = 16\nn_head = 4 #how many sections we divide each embedding, every head has 384/6 = 64 dimension\nn_layer = 6 #how many blocks\ndropout = 0.2 #20% nodes will drop to zero\ntorch.manual_seed(598) #manually set random number\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.747412Z","iopub.execute_input":"2023-04-30T17:52:28.747926Z","iopub.status.idle":"2023-04-30T17:52:28.763380Z","shell.execute_reply.started":"2023-04-30T17:52:28.747872Z","shell.execute_reply":"2023-04-30T17:52:28.761790Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x71e366fc3ef0>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Visualizing a sequence example ####","metadata":{}},{"cell_type":"code","source":"x= train_data[:block_size] #x= numbers in position 0-7 - input\ny= train_data[1:block_size+1] #y= numbers in position 1-8 - target\nprint(str(x))\nprint(str(y))\n\n#for t in range(block_size): #visualize context and target at each position\n    #context = x[:t+1]#all characters up to t (including t) in x list\n   # target = y[t]#t'th character in y list \n    #print(f\"when input is {context} the target is:{target}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.765718Z","iopub.execute_input":"2023-04-30T17:52:28.766261Z","iopub.status.idle":"2023-04-30T17:52:28.776521Z","shell.execute_reply.started":"2023-04-30T17:52:28.766209Z","shell.execute_reply":"2023-04-30T17:52:28.774781Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([59, 44, 63, 59,  0,  3, 19, 48])\ntensor([44, 63, 59,  0,  3, 19, 48, 57])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Getting samples from our dataset ### \nGetting random blocks/sequences from our dataset and stacking them into tensors","metadata":{}},{"cell_type":"code","source":"def get_batch(split):\n    data = train_data if split == \"train\" else val_data #validation or training?\n    ix = torch.randint(len(data) - block_size, (batch_size,))#generating 4 different random numbers between 0 and len(data)-8 \n    x = torch.stack([data[i:i+block_size] for i in ix])#get 8 characters after each of 4 random numbers\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])#same now, shifted one unit right \n    return x, y#returning both tensors as a tuple\n\n\n#VISUALIZING AN EXAMPLE\n\nxb, yb = get_batch('train')#rename x and y as xb and yb for the sake of clarity\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.782362Z","iopub.execute_input":"2023-04-30T17:52:28.782839Z","iopub.status.idle":"2023-04-30T17:52:28.823579Z","shell.execute_reply.started":"2023-04-30T17:52:28.782797Z","shell.execute_reply":"2023-04-30T17:52:28.821833Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8])\ntensor([[ 1, 52, 44,  1, 51, 44, 40, 61],\n        [43,  1, 41, 44, 45, 54, 57, 44],\n        [ 1, 40, 53, 43,  1, 51, 54, 40],\n        [57,  1, 52, 64,  1, 41, 54, 60]])\ntargets:\ntorch.Size([4, 8])\ntensor([[52, 44,  1, 51, 44, 40, 61, 44],\n        [ 1, 41, 44, 45, 54, 57, 44,  9],\n        [40, 53, 43,  1, 51, 54, 40, 59],\n        [ 1, 52, 64,  1, 41, 54, 60, 53]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Loss function ####\nEstimated loss: average predicted loss for training and validating data ","metadata":{}},{"cell_type":"code","source":"@torch.no_grad() #stop calculation of gradients \ndef estimate_loss():\n    out = {}  #initialize an empty dictionary\n    model.eval() #no changes in the model (only evaluation)\n    for split in [\"train\",\"val\"]: #loop over training and validation\n        losses = torch.zeros(eval_iters) #new tensor of zeros with length eval_iters\n        for k in range(eval_iters):#go through k number of batches\n            X, Y = get_batch(split) #batch of x and y sequences from val and train\n            logits, loss = model(X,Y)#run model and calculate loss value\n            losses[k] = loss.item()#add loss to losses\n        out[split] = losses.mean()#calculate mean\n    model.train()#back to training mode\n    return out\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.824850Z","iopub.execute_input":"2023-04-30T17:52:28.825331Z","iopub.status.idle":"2023-04-30T17:52:28.833545Z","shell.execute_reply.started":"2023-04-30T17:52:28.825279Z","shell.execute_reply":"2023-04-30T17:52:28.832240Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Single head self attention ###","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    \n    def __init__(self,head_size):\n        super().__init__()\n        #multiplying the tensors by some fixed weights\n        self.key = nn.Linear(n_embd, head_size, bias=False) # (B,T,C) --> (B,T,16)\n        self.query = nn.Linear(n_embd, head_size, bias=False) # (B,T,C) --> (B,T,16)\n        self.value = nn.Linear(n_embd, head_size, bias=False) # (B,T,C) --> (B,T,16)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) \n        #it is a tensor called trill of size block_size that has ones in the bottom triangle and zeros in the top\n        self.dropout = nn.Dropout(dropout)\n    def forward(self,x):#all tokens produce a key and a query \n        B,T,C = x.shape\n        k = self.key(x) #(B,T,32) (each key and query is made of 32 numbers)\n        q = self.query(x) #(B,T,32)\n        wei = q@k.transpose(-2,-1) * C**-0.5 # switch the last two (B,T,16) dimensions of k (B,T,16) @ (B,16,T) --> (B,T,T)\n        wei = wei.masked_fill(self.tril[:T,:T]== 0, float(\"-inf\"))\n        #tensor with bottom triangle of zeros, and upper of -inf (ignoring those tokens)\n        wei=F.softmax(wei, dim=-1) #-1 dimension = horizontally\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei@v #weight the value vectors of all the tokens \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.834936Z","iopub.execute_input":"2023-04-30T17:52:28.835401Z","iopub.status.idle":"2023-04-30T17:52:28.848485Z","shell.execute_reply.started":"2023-04-30T17:52:28.835352Z","shell.execute_reply":"2023-04-30T17:52:28.846922Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Multi-Head Self-Attention ###","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__() #initialize nn.Module\n        #head size is 8 and num_heads is 4 so stacking them would result in 32 (our n_embd)\n        self.heads = nn.ModuleList([Head(head_size) for _ in range (num_heads)]) \n        #create a list called heads which stores the output embeddings of the Head class for each of the heads\n        self.proj = nn.Linear(n_embd, n_embd) #linear transformation of the embeddings \n    def forward(self,x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1) #x is the sequences of tokens with their corresponding embeddings\n        out = self.proj(out) #pass output through linear layer\n        return out #put all the outputs from the list into a tensor\n    \n    #Ouput = tensor (B,T, embeddings of all the heads stacked together) which represents the\n    #context of the input","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.849944Z","iopub.execute_input":"2023-04-30T17:52:28.851410Z","iopub.status.idle":"2023-04-30T17:52:28.864446Z","shell.execute_reply.started":"2023-04-30T17:52:28.851352Z","shell.execute_reply":"2023-04-30T17:52:28.863288Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Feedforward ###\nFeedforward is a simple layer to add computation to the model (make the relation betweek k, q and v more complex). Adds **non-linearity**.\nAllow the model to learn more complex and **abstract representations** of the sequence, to improve the model's ability to understand and generate text.","metadata":{}},{"cell_type":"code","source":"#all tokens do independently \nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__() #call the nn.Module\n        self.net = nn.Sequential( #sequential pytorch module \n            nn.Linear(n_embd, 4* n_embd), #linear NN layer in which we add a 4x bigger layer\n            nn.ReLU(), #ReLU activation layer (all - numbers become 0)\n            nn.Linear(4* n_embd, n_embd), #back to original size\n            nn.Dropout(dropout), \n            #randomly set the weight of some neurons to zero to avoid overfitting.\n        )\n    \n    def forward(self,x):\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.865865Z","iopub.execute_input":"2023-04-30T17:52:28.866522Z","iopub.status.idle":"2023-04-30T17:52:28.885913Z","shell.execute_reply.started":"2023-04-30T17:52:28.866468Z","shell.execute_reply":"2023-04-30T17:52:28.884730Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Block ###","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head # 32 dimension/4 heads = (embedding dimension per head)\n        #communication\n        self.sa= MultiHeadAttention(n_head, head_size)\n        #computation\n        self.ffwd = FeedForward(n_embd)\n        #normalizing each embedding so that the embedding of each token (32 numbers) has mean 0 SD 1\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n    def forward(self,x):\n        #applying skip/residual connections to come back to the original embeddings\n        x = x + self.sa(self.ln1(x)) \n        x = x + self.ffwd(self.ln2(x))\n        return x ","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.888190Z","iopub.execute_input":"2023-04-30T17:52:28.888767Z","iopub.status.idle":"2023-04-30T17:52:28.906481Z","shell.execute_reply.started":"2023-04-30T17:52:28.888711Z","shell.execute_reply":"2023-04-30T17:52:28.904863Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Putting it all together ###","metadata":{}},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    \n    def __init__(self):#initializing the model\n        super().__init__() #initializing nn.Module superclass\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        #map each token to a dense vector (layer of neurons)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        #each position will get its own embedding vector\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range (n_layer)]) \n        #pass sequence through all blocks sequentially\n        self.ln_f = nn.LayerNorm(n_embd) #normalize the embeddings mean 0 SD 1 \n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    #index and target are both tensors of dimension (B,T)\n    def forward(self,idx,targets=None):\n        B,T = idx.shape\n        tok_emb = self.token_embedding_table(idx) # converting character index (B,T) -> embedding (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T)) \n        #torch.arange() assigns each position to an int\n        x = tok_emb + pos_emb#add the pos embedding to every batch of the token embedding\n        x = self.blocks(x) #apply the self attention\n        x = self.ln_f(x)\n        logits = self.lm_head(x) # embedding -> logits (B,T, vocab_size)\n        \n        if targets is None: #if there are no targets, no loss is calcualted\n            loss = None\n        else: \n            B,T,C = logits.shape #Reshaping our tensors to fit requirements for cross entropy\n            logits = logits.view(B*T, C) #stacking the logits tensors (reduce to 2 dimensions)\n            targets = targets.view(B*T) #stacking the targets tensors (reduce to 1 dimension)\n            loss = F.cross_entropy(logits, targets) #calculating loss\n        \n        return(logits, loss)\n    \n    def generate(self, idx, max_new_tokens):#takes in the index of tensors 4x8 (BxT)\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:] #crops the context to be max the size of block size\n            logits,loss = self(idx_cond) #make the predictions for the next character and ignore the loss\n            logits = logits[:,-1,:] #remove the last character (last letter of each sequence)\n            probs = F.softmax(logits, dim=-1) #pass the logits through the softmax\n            idx_next = torch.multinomial(probs,num_samples=1) #perform weighted prediction\n            idx = torch.cat((idx, idx_next), dim=1) #add predicted characters to the sequence and change t to t+1\n        return idx\n    \nmodel = BigramLanguageModel()\nlogits, loss = model(xb, yb)\nprint(logits.shape)\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:28.908546Z","iopub.execute_input":"2023-04-30T17:52:28.909111Z","iopub.status.idle":"2023-04-30T17:52:29.198636Z","shell.execute_reply.started":"2023-04-30T17:52:28.909052Z","shell.execute_reply":"2023-04-30T17:52:29.197053Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([32, 66])\ntensor(4.3175, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)#adam is an advanced optimizer","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:29.202463Z","iopub.execute_input":"2023-04-30T17:52:29.203503Z","iopub.status.idle":"2023-04-30T17:52:29.213712Z","shell.execute_reply.started":"2023-04-30T17:52:29.203406Z","shell.execute_reply":"2023-04-30T17:52:29.210540Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING #","metadata":{}},{"cell_type":"code","source":"batch_size = 32 \nfor steps in range(5000):\n    \n    if steps % eval_interval ==0: #sometimes calculate the estimated loss\n        losses = estimate_loss() #run estimate_loss() function and store in variable losses \n        print(f'step {steps}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n        #print losses rounded to 4 decimal places\n    \n    xb, yb = get_batch(\"train\") #get data from training data using our function\n    logits,loss = model(xb,yb) #instance of our Bigram Language model\n    optimizer.zero_grad(set_to_none=True) #previous gradients are set to zero\n    loss.backward() #evaluating loss\n    optimizer.step() #update parameters","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:52:29.217125Z","iopub.execute_input":"2023-04-30T17:52:29.218343Z","iopub.status.idle":"2023-04-30T18:02:07.016371Z","shell.execute_reply.started":"2023-04-30T17:52:29.218294Z","shell.execute_reply":"2023-04-30T18:02:07.015066Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"step 0: train loss 4.3507, val loss 4.3569\nstep 100: train loss 3.3863, val loss 3.4042\nstep 200: train loss 3.1229, val loss 3.1543\nstep 300: train loss 2.9082, val loss 2.9353\nstep 400: train loss 2.7825, val loss 2.8033\nstep 500: train loss 2.7082, val loss 2.7213\nstep 600: train loss 2.6465, val loss 2.6592\nstep 700: train loss 2.5960, val loss 2.6034\nstep 800: train loss 2.5593, val loss 2.5723\nstep 900: train loss 2.5263, val loss 2.5339\nstep 1000: train loss 2.4923, val loss 2.5159\nstep 1100: train loss 2.4779, val loss 2.4875\nstep 1200: train loss 2.4561, val loss 2.4673\nstep 1300: train loss 2.4465, val loss 2.4491\nstep 1400: train loss 2.4218, val loss 2.4382\nstep 1500: train loss 2.4102, val loss 2.4214\nstep 1600: train loss 2.4009, val loss 2.4056\nstep 1700: train loss 2.3793, val loss 2.3967\nstep 1800: train loss 2.3691, val loss 2.3859\nstep 1900: train loss 2.3584, val loss 2.3748\nstep 2000: train loss 2.3521, val loss 2.3784\nstep 2100: train loss 2.3404, val loss 2.3503\nstep 2200: train loss 2.3296, val loss 2.3445\nstep 2300: train loss 2.3232, val loss 2.3349\nstep 2400: train loss 2.3084, val loss 2.3238\nstep 2500: train loss 2.3055, val loss 2.3169\nstep 2600: train loss 2.2966, val loss 2.3150\nstep 2700: train loss 2.2842, val loss 2.3007\nstep 2800: train loss 2.2739, val loss 2.2892\nstep 2900: train loss 2.2674, val loss 2.2841\nstep 3000: train loss 2.2718, val loss 2.2841\nstep 3100: train loss 2.2652, val loss 2.2719\nstep 3200: train loss 2.2495, val loss 2.2650\nstep 3300: train loss 2.2385, val loss 2.2605\nstep 3400: train loss 2.2457, val loss 2.2555\nstep 3500: train loss 2.2305, val loss 2.2454\nstep 3600: train loss 2.2254, val loss 2.2479\nstep 3700: train loss 2.2187, val loss 2.2507\nstep 3800: train loss 2.2210, val loss 2.2366\nstep 3900: train loss 2.2071, val loss 2.2331\nstep 4000: train loss 2.2055, val loss 2.2268\nstep 4100: train loss 2.1943, val loss 2.2266\nstep 4200: train loss 2.1931, val loss 2.2208\nstep 4300: train loss 2.1983, val loss 2.2234\nstep 4400: train loss 2.1887, val loss 2.2130\nstep 4500: train loss 2.1832, val loss 2.2114\nstep 4600: train loss 2.1810, val loss 2.1971\nstep 4700: train loss 2.1736, val loss 2.2006\nstep 4800: train loss 2.1656, val loss 2.2059\nstep 4900: train loss 2.1725, val loss 2.1985\n","output_type":"stream"}]},{"cell_type":"code","source":"\n#print(decode(model.generate(context,idx=369,max_new_tokens=2000)[0].tolist()))\n#passing a context of zero\n#tensor shape (1, 1) filled with 0 = starting token\nprint(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)[0].tolist()))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:02:07.018711Z","iopub.execute_input":"2023-04-30T18:02:07.019897Z","iopub.status.idle":"2023-04-30T18:02:10.797808Z","shell.execute_reply.started":"2023-04-30T18:02:07.019814Z","shell.execute_reply":"2023-04-30T18:02:10.796585Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\nI and, anmpoly buth sownerere fuersearearet miong arounsse tilk\nSo thens Hosptreal this sheat go awelen aswing hir ham dis;son ha?\nNows GFough to le fits wich ther.\nHust PEd youse my kiDed\nUn,,\nAs wherdy lIUCfon dee saterd is farded, prot froomim thou een towno le ang thay:\nFirt, theeere a of you our detdang ted a this him Vimaty ae fiein thith ath you anlt be sownchat.\n\nKEd cho fut nour mur art i\n","output_type":"stream"}]}]}