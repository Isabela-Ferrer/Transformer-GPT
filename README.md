# Transformer-GPT
This code aims to replicate the code behind models that use the transformer model, like GPT. It uses Pytorch and Python.

I created the different layers of the transformer architecture, specifically for the decoder blocks, including the *self attention* layer, *multi-head attention* mechanism and the *feedforeward neural network*.

In the code, I also go through the training of the model as well as perform some predictions.

To train the model, I used the data frome Tiny Shakespeare. 

### All credits of the code go to this tuturial https://youtu.be/kCc8FmEb1nY
