# Transformer-GPT
This code aims to replicate the code behind models that use the transformer model, like GPT. It uses pytorch and python.

I created the different layers of the transformer architecture, specifically for the decoder blocks, including the self attention layer, multi-head attention mechanism and the feedforeward neural network.

To train the model, I used the data frome Tiny Shakespeare. 

### All credits of the code go to this tuturial https://youtu.be/kCc8FmEb1nY
